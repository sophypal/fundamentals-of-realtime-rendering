<div class="relative px-4 sm:px-6 lg:px-8">
    <Heading @title='Shaders' @subtitle='Shaders for the win!'>
        All 3D graphics programming APIs allow you to take control of the GPU. APIs pay for that freedom and flexiblity by being a
        little bit cumbersome to use. While in the past, these APIs were somewhat abstract, the trend is gravitating towards more
        low level APIs so developers can harness the full power of the hardware. To do anything in 3D nowadays requires some knowledge
        on writing shaders. Unless you're working in an engine that has powerful material editing workflows,
        more than likely you'll encounter scenarios that require custom shaders. Even then, the more you know about shaders,
        the more complex materials you can create. Let's start learning the basics.
    </Heading>
    <ProseMarkdown>
        ## What Are Shaders

        Shaders are programs that are run on the **GPU**. Depending on the **3D** graphics API, (**OpenGL**, **DirectX**, **Metal**, **Vulcan**), and what hardware,
        you probably have to provide one as no defaults are given. If you're using an engine or higher-level API, like **ThreeJS**, then
        this is done for you. But we're not here to discuss someone else's implementation.

        Shaders are compiled by the **GPU**. While it's possible to precompile the shaders, you typically want to compile it dynamically
        either during loading, through updates to user settings and configurations, or god-forbid during gameplay. Due to the variability
        of **PC** hardware, running code compiled on one architecture is bad for many reasons. Driver updates might invalidate or even break
        your code. **GPUs** can be changed. Some applications are smart and will compile shaders only once but performance is not the
        concern of these tutorials.

        Shaders are written in a **C-style** language optimized for vectorized architectures. They contain several built-in functions for
        math operations, like **normalize**, **reflect** for vector math; **sin**, **cos** for trig, and **mix** for linear interpolation, and
        many many more.

        There are 2 dominant shader types, vertex and fragment. Actually there are geometry and tesselation shaders but I'll limit the
        discussion on vertex and fragment shaders for now.

        All the examples here are written in **GLSL** using the **ThreeJS** framework because this is on the Web but what we will discuss here
        can be applicable to **OpenGL** and even other shader languages like **HLSL** (**DirectX**) because most **GPUs** are architectured the same way.
        I'll mention some terminology that might be more relevent to **OpenGL** but the concepts are almost identical in other APIs.

        ## Lifecycle

        Shaders are resources just like any other asset. They start out as text which needs to be read in. If the text is in a file, the
        application needs to take care of reading them from a file. Once read in, they need to be compiled. In **OpenGL**, shaders are attached
        to programs and only one program can be active at any point and time.

        Shaders will be used during a drawing operation. Drawing operations are issued from the application, along with basic inputs the
        shader needs to operate on. From that point on, multiple shader invocations will be made by the **GPU**, outputing the results to the frame
        buffer. This cycle starts over again on the next draw call. If using double buffering, the results don't appear on screen until the
        buffer is swapped. Based on the discussion earlier on the graphics pipeline, the shader will typically follow this order.

        1. Read shaders from a file
        2. Compile shaders into shader programs
        3. Loop through scene objects

            * set object's perferred shader as active
            * bind any data the shader needs (vertices, indices, constants)
            * issue draw call

        4. Flush commands (commands are sent to the **GPU**)

            * vertex data is transferred to gpu memory
            * draw commands are issued which tell the **GPU** how to interpet the data
            * multiple vertex shaders are invoked for each vertex in the object and they all run to completion
            * multiple fragment shaders are invoked for each pixel in the viewport

        5. swap back frame buffer with front buffer
        6. Start over again with 3

        For performance reasons, you might not see this order followed to the tee in practice. **GPU** latency is quite slow so data transfers are
        typically avoided during the render loop. Setting up **GPU** buffers for vertex data shouldn't be done per frame but passing shader **uniforms** are normally
        done in the loop.

        ## Inputs

        Vertex shaders operate on vertices and fragment shaders operate on pixels. We learned that the rasterizer supplies the inputs to the
        fragment shader but where does the inputs for vertex shaders come from? They come from the application, in user code. Since **GPUs** are
        high-throughput low-latency devices, it's not very efficient to store the vertex data in main memory. Data would need to be
        transferred over to **GPU** memory for every draw call, for every frame.

        Instead, buffers can be reserved in **GPU** memory and we can use this to dump whatever data we need, in whatever form we need it in.
        This really only should be done once, when the object's are first loaded. If we have multiple objects, we reserve more **GPU** memory
        and when we're ready to draw them, we simply tell the **GPU** which buffer is active.

        There are 2 kinds of inputs for a vertex shader:

        * **Attributes**
        * **Uniforms**


        ### Attributes

        **Attributes** contain per vertex information. For example, **position**, **normal**, and texture **UVs** are all per vertex information. You can
        have color, if you want to support vertex coloring. This means that for each vertex, there exist one position vector, one normal
        vector, one uv coordinate vector, and one color vector.

        **Attributes** aren't normally updated during the game loop, especially for static objects. For dynamic objects whose shape can change
        overtime, or if streaming vertex data that is normally too large to fit in **GPU** memory, you can update this buffer over time but
        it comes at a cost.

        When reserving **GPU** memory, we need to let the GPU know how the data is laid out. Let's look at an example shader that needs position
        and color as inputs.

        ```glsl
        #version 330 core

        layout (location = 0) in vec3 aPos;
        layout (location = 1) in vec3 aColor;

        out vec3 vColor;

        void main()
        {
            gl_Position = vec(aPos, 1.0);
            vColor = aColor;
        }
        ```

        This shader is expecting **aPos** to come from the first position of this buffer, and **aColor** second. **OpenGL** needs to know about the offset and stride
        to calculate where those attributes are. **aPos** will take 12 bytes because its a **vec3** (contains 3 **float**s). **aColor** is a similar
        data type so it takes 12 bytes. When you layout a single vertex in memory it will look like the diagram below.

        </ProseMarkdown>
        <ProseFigure
            @figureId='1'
            @figureCaption='This is an example of vertex attributes interleaved into a single vertex buffer. Binding a variable to each attributes just requires the offset into this buffer as well as the amount of bytes to get to the next vertex'
            @imageUrl={{root-url 'figures/vertex-layout-detailed.png'}}
            @vertical={{true}}
        />
        <ProseMarkdown>
        The offset describes how many bytes from the beginning of the first vertex attribute.

        NOTE: Older **OpenGL** versions pre v3 could not use the keyword **layout** in the vertex shader. The order was implied from the order of declaration.
        Keywords like **attribute**, **varying** were used instead of **in** and **out** to show they were inputs or outputs. **WebGL 1.0** is based on **OpenGL**
        **ES 2.0** spec so it cannot use **layout**.

        ### Uniforms

        The other form of input is called a **uniform**. A **uniform**, like the name implies, is constant over all vertices. They don't change from one shader
        invocation to the next, unlike **vertex attributes**. They are global and can be referenced within the vertex shader or fragment shader. They are often
        set per frame.

        **Uniforms** are often used to apply textures. **Uniforms** can also be used to animate by passing in elapsed time as a parameter. They can also make shaders more configurable.

        ## Outputs

        Fragment shader inputs are a bit different from vertex shader inputs. For one, these shaders don't take in **attributes**. They can still take **uniforms**
        but the pixel inputs come from the rasterizer. The rasterizer will pass along the outputs of the vertex data. Since there are more
        pixels then vertices, in most cases, the vertex data is interpolated across all pixels. This means that every pixel will have per-pixel
        position data, and color data.

        In **OpenGL** v3, use **out** to specify which variable becomes an input for the fragment shader. In previous versions, use **varying**. Vertex shaders should
        also output a position to the next stage in the graphics pipeline. You set the global variable **gl_Position** to do this. For fragment shaders, you should output
        to **gl_FragColor** or define our own **out** variable if using version 3 and above.
    </ProseMarkdown>
</div>